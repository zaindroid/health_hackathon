# ğŸš€ Cartesia Sonic TTS + Latency Optimization Guide

## âœ… What's Been Added

I've integrated **Cartesia Sonic TTS** - an ultra-low latency text-to-speech service that's **much faster** than Web Speech API!

### Features

âœ… **Ultra-low latency** (50-200ms vs 1-2s for Web Speech API)
âœ… **Natural-sounding voices**
âœ… **Consistent quality**
âœ… **No browser dependencies**
âœ… **Server-side generation**

---

## ğŸ™ï¸ Available Voices

Your system is configured with **"Barbershop Man"** (warm, friendly male voice).

**Other available voices:**
- `Doctor Mischief` - Authoritative male (great for medical!)
- `Calm Lady` - Soothing female voice
- `British Lady` - Professional British accent
- `Friendly Reading Man` - Clear, articulate

---

## ğŸš€ How to Test

**1. Restart Your Backend:**

```bash
cd ~/healthy_hack/backend
npm run dev
```

**You should see:**
```
âœ… Cartesia Sonic TTS Provider initialized
   Voice: Barbershop Man
```

**2. Test the Voice Agent:**

1. Refresh browser: http://localhost:5173
2. Click "Start Talking"
3. Say: "Tell me about the heart"
4. **Notice the much faster voice response!** ğŸ‰

---

## ğŸ¯ Latency Breakdown

### Current Pipeline Latency:

| Step | Latency | Can Optimize? |
|------|---------|---------------|
| **1. Voice â†’ Deepgram** | 100-300ms | âš ï¸ Network dependent |
| **2. Deepgram STT** | 50-100ms | âœ… Already optimized |
| **3. Qwen3 Generation** | 10-30s | âœ… **YES - See below** |
| **4. Cartesia TTS** | 100-200ms | âœ… Already fast! |
| **5. Audio playback** | 50ms | âœ… Already optimized |

**Total:** ~10-31 seconds (mostly Qwen3!)

---

## âš¡ How to Reduce Qwen3 Latency

### Option 1: Use Smaller Model (Recommended!)

Switch to the 7B model for 3-5x faster responses:

**Edit `backend/.env`:**
```env
LOCAL_MODEL_NAME=qwen2.5-coder:7b-instruct
```

**Speed improvement:** 10-30s â†’ 3-8s ğŸš€

**Trade-off:** Slightly less detailed responses, but still excellent quality!

---

### Option 2: Reduce Max Tokens

**Edit `backend/llm/local.ts` (line 57):**
```typescript
max_tokens: 200,  // Reduced from 500
```

**Speed improvement:** Responses finish 2-3x faster

**Trade-off:** Shorter responses (but you asked for <50 words anyway!)

---

### Option 3: Increase Temperature (Counter-intuitive!)

Higher temperature can sometimes generate faster:

**Edit `backend/llm/local.ts` (line 56):**
```typescript
temperature: 0.9,  // Increased from 0.7
```

**Speed improvement:** Slight improvement, more variety

---

### Option 4: Use Streaming (Advanced)

**TODO:** Implement streaming responses from Qwen3

This would allow:
- Start speaking before full response generated
- **Perceived latency**: Near instant!
- **Technical complexity**: High

---

## ğŸ›ï¸ Change Cartesia Voice

**Edit `backend/tts/cartesia.ts` (line 14):**

```typescript
// For medical professional voice:
private voiceId = '638efaaa-4d0c-442e-b701-3fae16aad012'; // Doctor Mischief

// For calm female voice:
private voiceId = '87748186-23bb-4158-a1eb-332911b0b708'; // Calm Lady

// For British accent:
private voiceId = '79a125e8-cd45-4c13-8a67-188112f4dd22'; // British Lady
```

Then restart backend.

---

## ğŸ“Š Recommended Optimization Path

### For Best Balance (Speed + Quality):

**1. Switch to 7B model** (3-8s responses):
```env
LOCAL_MODEL_NAME=qwen2.5-coder:7b-instruct
```

**2. Reduce max tokens** (faster completion):
```typescript
max_tokens: 200
```

**3. Keep Cartesia** (already optimal!)

**Expected total latency:** ~4-9 seconds ğŸ‰

---

### For Maximum Speed (Sacrifice Quality):

**1. Use 7B model**
**2. Max tokens = 100**
**3. Temperature = 0.9**

**Expected latency:** ~2-5 seconds

**Trade-off:** Less detailed, more varied responses

---

## ğŸ”¬ Advanced: Streaming Implementation

To achieve **near-instant perceived latency**, you could implement:

### Backend Changes:
1. Stream tokens from Qwen3 as they're generated
2. Send partial text to Cartesia in chunks
3. Stream audio back to frontend

### Expected Result:
- First words spoken in 2-3 seconds
- Full response over 10-20 seconds
- **Feels instant to user!**

**Complexity:** High (requires WebSocket streaming, partial audio handling)

---

## ğŸ¯ Quick Wins (Do These Now!)

### 1. Switch to 7B Model

```bash
cd ~/healthy_hack/backend
nano .env
# Change: LOCAL_MODEL_NAME=qwen2.5-coder:7b-instruct
# Save and restart
```

**Impact:** 10-30s â†’ 3-8s

### 2. Reduce Max Tokens

```bash
nano llm/local.ts
# Line 57: change 500 to 200
# Save and restart
```

**Impact:** Additional 2-3x speedup

---

## ğŸ“ˆ Performance Comparison

| Configuration | Response Time | Quality | Cost |
|---------------|---------------|---------|------|
| **Qwen3 30B + 500 tokens** | 10-30s | Excellent | Free |
| **Qwen3 30B + 200 tokens** | 5-15s | Very Good | Free |
| **Qwen2.5 7B + 500 tokens** | 3-8s | Good | Free |
| **Qwen2.5 7B + 200 tokens** | 2-5s | Good | Free |
| **OpenAI GPT-4o-mini** | 1-3s | Excellent | $0.15/1M |

---

## ğŸ‰ Summary

**What you have now:**
âœ… Ultra-fast Cartesia TTS (100-200ms)
âœ… Qwen3-30B for high quality (10-30s)
âœ… Full control over latency

**Quick wins:**
1. Switch to 7B model â†’ 3-8s responses
2. Reduce tokens â†’ 2-3x faster
3. Enjoy Cartesia's natural voice!

**Advanced option:**
- Implement streaming for perceived instant responses

---

## ğŸš€ Ready to Test!

**Restart your backend** and try it out! You should notice Cartesia's voice is **much more natural** and **faster** than the browser TTS!

```bash
cd ~/healthy_hack/backend
npm run dev
```

Then speak to it and enjoy the improved experience! ğŸ‰
